#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Autodistill vs Ground Truth ë¶„ë¥˜ ì •í™•ë„ ë¶„ì„
"""

import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.metrics import precision_recall_fscore_support
import argparse
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)
plt.rcParams['font.family'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def extract_frame_info_from_filename(filename):
    """
    íŒŒì¼ëª…ì—ì„œ í”„ë ˆì„ ì •ë³´ ì¶”ì¶œ
    ì˜ˆ: 'G_1_2_frame_0073_obj2_cls3_unknown_class_3.png' -> 'G_1_2_frame_0073'
    """
    # _objê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ì˜ ë¶€ë¶„ì„ ì¶”ì¶œ
    match = re.match(r'(.+?)_obj\d+', filename)
    if match:
        return match.group(1)
    return None

def extract_object_index_from_filename(filename):
    """
    íŒŒì¼ëª…ì—ì„œ ê°ì²´ ì¸ë±ìŠ¤ ì¶”ì¶œ
    ì˜ˆ: 'G_1_2_frame_0073_obj2_cls3_unknown_class_3.png' -> 2
    """
    match = re.search(r'_obj(\d+)_', filename)
    if match:
        return int(match.group(1))
    return None

def get_ground_truth_mapping(gt_dir):
    """
    Ground Truth ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ ë§¤í•‘ ìƒì„±
    Returns: {(frame_name, obj_index): gt_class}
    """
    logger.info("Ground Truth ë§¤í•‘ ìƒì„± ì¤‘...")
    
    gt_mapping = {}
    class_folders = {
        'Class_0': 'Class_0',
        'Class_1': 'Class_1', 
        'Class_2': 'Class_2',
        'Class_3': 'Class_3',
        'unknown_egifence': 'unknown_egifence',
        'unknown_human': 'unknown_human',
        'unknown_road': 'unknown_road',
        'unknown_none': 'unknown_none'
    }
    
    gt_stats = Counter()
    
    for folder_name, gt_class in class_folders.items():
        folder_path = os.path.join(gt_dir, folder_name)
        if not os.path.exists(folder_path):
            logger.warning(f"Ground Truth í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {folder_path}")
            continue
            
        for filename in os.listdir(folder_path):
            if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                continue
                
            frame_name = extract_frame_info_from_filename(filename)
            obj_index = extract_object_index_from_filename(filename)
            
            if frame_name and obj_index is not None:
                key = (frame_name, obj_index)
                gt_mapping[key] = gt_class
                gt_stats[gt_class] += 1
    
    logger.info("Ground Truth í†µê³„:")
    for gt_class, count in sorted(gt_stats.items(), key=lambda x: (isinstance(x[0], str), x[0])):
        logger.info(f"  {gt_class}: {count}ê°œ")
    
    return gt_mapping

def get_autodistill_mapping(autodistill_dir):
    """
    Autodistill ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ ë§¤í•‘ ìƒì„±
    Returns: {(frame_name, obj_index): autodistill_class}
    """
    logger.info("Autodistill ë§¤í•‘ ìƒì„± ì¤‘...")
    
    autodistill_mapping = {}
    autodistill_stats = Counter()
    
    for class_id in [0, 1, 2, 3]:
        folder_path = os.path.join(autodistill_dir, f'Class_{class_id}')
        class_label = f'Class_{class_id}'  # ë¬¸ìì—´ë¡œ ë³€í™˜
        
        if not os.path.exists(folder_path):
            logger.warning(f"Autodistill í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {folder_path}")
            continue
            
        for filename in os.listdir(folder_path):
            if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                continue
                
            frame_name = extract_frame_info_from_filename(filename)
            obj_index = extract_object_index_from_filename(filename)
            
            if frame_name and obj_index is not None:
                key = (frame_name, obj_index)
                autodistill_mapping[key] = class_label
                autodistill_stats[class_label] += 1
    
    logger.info("Autodistill í†µê³„:")
    for class_id, count in sorted(autodistill_stats.items()):
        logger.info(f"  {class_id}: {count}ê°œ")
    
    return autodistill_mapping

def create_confusion_matrix_data(autodistill_mapping, gt_mapping):
    """
    Confusion matrix ë°ì´í„° ìƒì„±
    """
    logger.info("Confusion matrix ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ê³µí†µ í‚¤ ì°¾ê¸°
    common_keys = set(autodistill_mapping.keys()) & set(gt_mapping.keys())
    logger.info(f"ë§¤ì¹­ëœ ì´ë¯¸ì§€ ê°œìˆ˜: {len(common_keys)}ê°œ")
    
    if len(common_keys) == 0:
        logger.error("ë§¤ì¹­ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None, None, None
    
    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    y_pred = []
    y_true = []
    detailed_data = []
    
    for key in common_keys:
        autodistill_class = autodistill_mapping[key]
        gt_class = gt_mapping[key]
        
        y_pred.append(autodistill_class)
        y_true.append(gt_class)
        
        detailed_data.append({
            'frame_name': key[0],
            'object_index': key[1],
            'autodistill_prediction': autodistill_class,
            'ground_truth': gt_class,
            'correct': autodistill_class == gt_class
        })
    
    return y_pred, y_true, detailed_data

def calculate_metrics(y_true, y_pred, labels):
    """
    ì •í™•ë„ ë©”íŠ¸ë¦­ ê³„ì‚°
    """
    logger.info("ì •í™•ë„ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
    
    # ì „ì²´ ì •í™•ë„
    accuracy = accuracy_score(y_true, y_pred)
    
    # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, labels=labels, average=None, zero_division=0
    )
    
    # ë§¤í¬ë¡œ í‰ê· 
    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(
        y_true, y_pred, labels=labels, average='macro', zero_division=0
    )
    
    # ê°€ì¤‘ í‰ê· 
    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(
        y_true, y_pred, labels=labels, average='weighted', zero_division=0
    )
    
    metrics = {
        'accuracy': accuracy,
        'macro_precision': macro_precision,
        'macro_recall': macro_recall,
        'macro_f1': macro_f1,
        'weighted_precision': weighted_precision,
        'weighted_recall': weighted_recall,
        'weighted_f1': weighted_f1,
        'class_metrics': {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'support': support
        }
    }
    
    return metrics

def plot_confusion_matrix(y_true, y_pred, labels, output_path):
    """
    Confusion matrix ì‹œê°í™”
    """
    logger.info("Confusion matrix ì‹œê°í™” ìƒì„± ì¤‘...")
    
    # Confusion matrix ê³„ì‚°
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    
    # ì •ê·œí™”ëœ confusion matrixë„ ê³„ì‚°
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # 2ê°œì˜ subplot ìƒì„±
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
    
    # ì›ë³¸ confusion matrix
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=labels, yticklabels=labels, ax=ax1)
    ax1.set_title('Confusion Matrix (Count)', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Ground Truth', fontsize=12)
    ax1.set_ylabel('Autodistill Prediction', fontsize=12)
    
    # ì •ê·œí™”ëœ confusion matrix
    sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',
                xticklabels=labels, yticklabels=labels, ax=ax2)
    ax2.set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Ground Truth', fontsize=12)
    ax2.set_ylabel('Autodistill Prediction', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"Confusion matrix ì €ì¥ë¨: {output_path}")
    
    return cm, cm_normalized

def save_detailed_results(detailed_data, metrics, cm, cm_normalized, labels, output_dir):
    """
    ìƒì„¸ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
    """
    logger.info("ìƒì„¸ ê²°ê³¼ CSV ì €ì¥ ì¤‘...")
    
    # 1. ìƒì„¸ ì˜ˆì¸¡ ê²°ê³¼
    detailed_df = pd.DataFrame(detailed_data)
    detailed_csv_path = os.path.join(output_dir, 'detailed_predictions.csv')
    detailed_df.to_csv(detailed_csv_path, index=False, encoding='utf-8-sig')
    
    # 2. Confusion matrix
    cm_df = pd.DataFrame(cm, index=labels, columns=labels)
    cm_csv_path = os.path.join(output_dir, 'confusion_matrix.csv')
    cm_df.to_csv(cm_csv_path, encoding='utf-8-sig')
    
    # 3. ì •ê·œí™”ëœ confusion matrix
    cm_norm_df = pd.DataFrame(cm_normalized, index=labels, columns=labels)
    cm_norm_csv_path = os.path.join(output_dir, 'confusion_matrix_normalized.csv')
    cm_norm_df.to_csv(cm_norm_csv_path, encoding='utf-8-sig')
    
    # 4. ì „ì²´ ë©”íŠ¸ë¦­
    metrics_data = {
        'Metric': ['Accuracy', 'Macro Precision', 'Macro Recall', 'Macro F1',
                   'Weighted Precision', 'Weighted Recall', 'Weighted F1'],
        'Value': [metrics['accuracy'], metrics['macro_precision'], metrics['macro_recall'],
                  metrics['macro_f1'], metrics['weighted_precision'], metrics['weighted_recall'],
                  metrics['weighted_f1']]
    }
    
    # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ ì¶”ê°€
    for i, label in enumerate(labels):
        metrics_data['Metric'].extend([
            f'{label}_Precision', f'{label}_Recall', f'{label}_F1', f'{label}_Support'
        ])
        metrics_data['Value'].extend([
            metrics['class_metrics']['precision'][i],
            metrics['class_metrics']['recall'][i],
            metrics['class_metrics']['f1'][i],
            metrics['class_metrics']['support'][i]
        ])
    
    metrics_df = pd.DataFrame(metrics_data)
    metrics_csv_path = os.path.join(output_dir, 'accuracy_metrics.csv')
    metrics_df.to_csv(metrics_csv_path, index=False, encoding='utf-8-sig')
    
    # 5. í´ë˜ìŠ¤ë³„ ìš”ì•½ í†µê³„
    summary_data = []
    for i, label in enumerate(labels):
        correct = sum(1 for d in detailed_data 
                     if d['autodistill_prediction'] == label and d['correct'])
        total_predicted = sum(1 for d in detailed_data 
                             if d['autodistill_prediction'] == label)
        total_actual = sum(1 for d in detailed_data 
                          if d['ground_truth'] == label)
        
        summary_data.append({
            'Class': label,
            'Precision': metrics['class_metrics']['precision'][i],
            'Recall': metrics['class_metrics']['recall'][i],
            'F1-Score': metrics['class_metrics']['f1'][i],
            'Support': metrics['class_metrics']['support'][i],
            'Correctly_Predicted': correct,
            'Total_Predicted': total_predicted,
            'Total_Actual': total_actual
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_csv_path = os.path.join(output_dir, 'class_summary.csv')
    summary_df.to_csv(summary_csv_path, index=False, encoding='utf-8-sig')
    
    logger.info(f"CSV íŒŒì¼ë“¤ì´ ì €ì¥ë¨: {output_dir}")
    
    return {
        'detailed': detailed_csv_path,
        'confusion_matrix': cm_csv_path,
        'confusion_matrix_norm': cm_norm_csv_path,
        'metrics': metrics_csv_path,
        'summary': summary_csv_path
    }

def print_console_summary(metrics, cm, labels, detailed_data):
    """
    ì½˜ì†”ì— ìš”ì•½ ì •ë³´ ì¶œë ¥
    """
    print("\n" + "="*80)
    print("AUTODISTILL vs GROUND TRUTH ë¶„ë¥˜ ì •í™•ë„ ë¶„ì„ ê²°ê³¼")
    print("="*80)
    
    # ì „ì²´ ì •í™•ë„
    print(f"\nğŸ“Š ì „ì²´ ì •í™•ë„: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
    print(f"ğŸ“Š ì´ ë§¤ì¹­ëœ ì´ë¯¸ì§€: {len(detailed_data)}ê°œ")
    
    # ë§¤í¬ë¡œ/ê°€ì¤‘ í‰ê· 
    print(f"\nğŸ“ˆ Macro Average:")
    print(f"   - Precision: {metrics['macro_precision']:.4f}")
    print(f"   - Recall: {metrics['macro_recall']:.4f}")
    print(f"   - F1-Score: {metrics['macro_f1']:.4f}")
    
    print(f"\nğŸ“ˆ Weighted Average:")
    print(f"   - Precision: {metrics['weighted_precision']:.4f}")
    print(f"   - Recall: {metrics['weighted_recall']:.4f}")
    print(f"   - F1-Score: {metrics['weighted_f1']:.4f}")
    
    # í´ë˜ìŠ¤ë³„ ìƒì„¸ ì •ë³´
    print(f"\nğŸ“‹ í´ë˜ìŠ¤ë³„ ìƒì„¸ ì •ë³´:")
    print("-" * 80)
    print(f"{'Class':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}")
    print("-" * 80)
    
    for i, label in enumerate(labels):
        precision = metrics['class_metrics']['precision'][i]
        recall = metrics['class_metrics']['recall'][i]
        f1 = metrics['class_metrics']['f1'][i]
        support = int(metrics['class_metrics']['support'][i])
        
        print(f"{str(label):<15} {precision:<10.4f} {recall:<10.4f} {f1:<10.4f} {support:<10}")
    
    # Confusion Matrix ìš”ì•½
    print(f"\nğŸ“Š Confusion Matrix (ì‹¤ì œ â†’ ì˜ˆì¸¡):")
    print("-" * 60)
    
    # í—¤ë”
    header = "ì‹¤ì œ\\ì˜ˆì¸¡".ljust(15)
    for label in labels:
        header += str(label).ljust(12)
    print(header)
    print("-" * 60)
    
    # ê° í–‰
    for i, true_label in enumerate(labels):
        row = str(true_label).ljust(15)
        for j, pred_label in enumerate(labels):
            row += str(cm[i][j]).ljust(12)
        print(row)
    
    # íŠ¹ë³„íˆ ì˜ëª» ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤ë“¤
    print(f"\nğŸš¨ ì£¼ìš” ì˜¤ë¶„ë¥˜ íŒ¨í„´:")
    misclassified = {}
    for data in detailed_data:
        if not data['correct']:
            key = (data['ground_truth'], data['autodistill_prediction'])
            misclassified[key] = misclassified.get(key, 0) + 1
    
    sorted_misclassified = sorted(misclassified.items(), 
                                  key=lambda x: x[1], reverse=True)[:5]
    
    for (true_class, pred_class), count in sorted_misclassified:
        print(f"   {true_class} â†’ {pred_class}: {count}ê°œ")

def main():
    parser = argparse.ArgumentParser(description='Autodistill vs Ground Truth ì •í™•ë„ ë¶„ì„')
    parser.add_argument('--category-path', default='data/test_category',
                        help='ì¹´í…Œê³ ë¦¬ ê²½ë¡œ (ê¸°ë³¸: data/test_category)')
    parser.add_argument('--output-dir', default='analysis_results',
                        help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: analysis_results)')
    parser.add_argument('--verbose', action='store_true',
                        help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ê²½ë¡œ ì„¤ì •
    autodistill_dir = os.path.join(args.category_path, '6.preprocessed')
    gt_dir = os.path.join(args.category_path, '7.results', 'ground_truth')
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_subdir = os.path.join(args.output_dir, f'analysis_{timestamp}')
    os.makedirs(output_subdir, exist_ok=True)
    
    print(f"ë¶„ì„ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Autodistill ë””ë ‰í† ë¦¬: {autodistill_dir}")
    print(f"Ground Truth ë””ë ‰í† ë¦¬: {gt_dir}")
    print(f"ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {output_subdir}")
    
    try:
        # 1. ë§¤í•‘ ìƒì„±
        autodistill_mapping = get_autodistill_mapping(autodistill_dir)
        gt_mapping = get_ground_truth_mapping(gt_dir)
        
        # 2. Confusion matrix ë°ì´í„° ìƒì„±
        y_pred, y_true, detailed_data = create_confusion_matrix_data(
            autodistill_mapping, gt_mapping)
        
        if y_pred is None:
            logger.error("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 3. ë¼ë²¨ ì •ì˜ (ìˆœì„œ ì¤‘ìš”)
        all_labels = sorted(set(y_true + y_pred), key=lambda x: (isinstance(x, str), x))
        
        # 4. ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = calculate_metrics(y_true, y_pred, all_labels)
        
        # 5. Confusion matrix ì‹œê°í™”
        cm_png_path = os.path.join(output_subdir, 'confusion_matrix.png')
        cm, cm_normalized = plot_confusion_matrix(y_true, y_pred, all_labels, cm_png_path)
        
        # 6. CSV íŒŒì¼ ì €ì¥
        csv_paths = save_detailed_results(detailed_data, metrics, cm, cm_normalized, 
                                        all_labels, output_subdir)
        
        # 7. ì½˜ì†” ìš”ì•½ ì¶œë ¥
        print_console_summary(metrics, cm, all_labels, detailed_data)
        
        # 8. íŒŒì¼ ê²½ë¡œ ì¶œë ¥
        print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        print(f"   - Confusion Matrix (PNG): {cm_png_path}")
        for name, path in csv_paths.items():
            print(f"   - {name.replace('_', ' ').title()} (CSV): {path}")
        
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
    except Exception as e:
        logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    main() 